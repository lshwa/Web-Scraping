from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import time

# 1. ì…€ë ˆë‹ˆì›€ ì„¤ì •
options = webdriver.ChromeOptions()
options.add_argument('--headless')  # ì°½ ì—†ì´ ì‹¤í–‰
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# 2. í¬ë¡¤ë§í•  í˜ì´ì§€ ë²”ìœ„ ì„¤ì •
base_url = "https://www.paxnet.co.kr/tbbs/list?tbbsType=L&id=N11023&page={}"
titles, writers, views, likes, dates = [], [], [], [], []

# 3. í˜ì´ì§€ ìˆœíšŒ
for page in range(1, 11):
    url = base_url.format(page)
    print(f"ğŸ“„ {page}í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
    driver.get(url)
    time.sleep(2)

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    posts = soup.select("ul#comm-list li")  # ê° ê²Œì‹œê¸€ì€ lië¡œ ì‹œì‘

    for post in posts:
        try:
            title = post.select_one("div.title")
            writer = post.select_one("div.write")
            view = post.select_one("div.viewer")
            like = post.select_one("div.like")
            date = post.select_one("div.date")

            # None ì²´í¬
            if None in (title, writer, view, like, date):
                continue

            titles.append(title.text.strip())
            writers.append(writer.text.strip())
            views.append(view.text.strip())
            likes.append(like.text.strip())
            dates.append(date.text.strip())

        except Exception as e:
            print(f"âŒ íŒŒì‹± ì˜¤ë¥˜: {e}")
            continue

# 4. ì¢…ë£Œ ë° ì €ì¥
driver.quit()

df = pd.DataFrame({
    "title": titles,
    "writer": writers,
    "views": views,
    "likes": likes,
    "date": dates
})
df.to_csv("paxnet_posts.csv", index=False, encoding='utf-8-sig')
print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(df)}ê±´ â paxnet_posts.csv ì €ì¥ë¨")