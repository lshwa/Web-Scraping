import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

base_url = "https://finance.naver.com/item/board.nhn?code=005930&page={}"
titles, writers, views, likes, dates = [], [], [], [], []

for page in range(1, 11):  # 1~10í˜ì´ì§€
    url = base_url.format(page)
    print(f"ğŸ“„ {page}í˜ì´ì§€ ìš”ì²­ ì¤‘...")
    headers = {
        "User-Agent": "Mozilla/5.0"
    }
    res = requests.get(url, headers=headers)
    res.encoding = 'euc-kr'  # ë„¤ì´ë²„ ê¸ˆìœµì€ euc-kr ì¸ì½”ë”© ì‚¬ìš©
    soup = BeautifulSoup(res.text, 'html.parser')

    rows = soup.select("table.type2 tr")

    for row in rows:
        cols = row.find_all("td")
        if len(cols) != 6:
            continue  # ê²Œì‹œê¸€ rowë§Œ ì¶”ì¶œ

        try:
            title = cols[1].get_text(strip=True)
            writer = cols[2].get_text(strip=True)
            view = cols[3].get_text(strip=True)
            like = cols[4].get_text(strip=True)
            date = cols[5].get_text(strip=True)

            titles.append(title)
            writers.append(writer)
            views.append(view)
            likes.append(like)
            dates.append(date)
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    time.sleep(1)

# ë°ì´í„° ì €ì¥
df = pd.DataFrame({
    "title": titles,
    "writer": writers,
    "views": views,
    "likes": likes,
    "date": dates
})

df.to_csv("naver_samsung_board.csv", index=False, encoding='utf-8-sig')
print(f"âœ… ì €ì¥ ì™„ë£Œ! ì´ {len(df)}ê±´ â naver_samsung_board.csv")